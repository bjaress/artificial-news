[i] scrape wikipedia using HTML, not wiki markup
  - too many random, quirky templates
  [x] separate branch
    html-based-scraping
  [x] set up Behave
    [x] replace Dockerfile.tests
      [x] /features
    [x] requirements-integration.txt
    [x] environment.py
      https://behave.readthedocs.io/en/stable/tutorial.html#environmental-controls
      [x] url lookups
      [x] poll wiremock ready
  [i] convert happy path integration test from cucumber to behave
      - convert from cucumber to behave as is
      [x] confirm all mocks up
      [x] mock news item
        - helpers that populate context
        - helper that syncs mock to context
        [x] headline with links
          [x] mock wikipedia headline
          [x] expected spreaker episode title (unabridged)
        [x] linked articles
          [x] expected wikipedia request info
          [x] mock wikipedia response info
          [x] expected tts request fragment
        [x] spreaker
          [x] mock spreaker response (no existing episodes)
            [x] store mock in context
          [x] sync to wiremock
        [x] google
          [x] mock google tts response
            [x] dummy mp3 data
      [x] trigger call to app
        [x] create payload
        [x] send
      [x] expect calls to wikipedia
        [x] headlines
        [x] articles in context
      [x] expect calls to tts
        [x] combination of title and article fragments from context
      [i] expect audio upload to spreaker
        [ ] parse multipart form data
        [ ] abridged version of title from context
        [ ] mock mp3 data from context
        [ ] show, other metadata
    [ ] expect success from app
  [ ] convert cleanup tests from cucumber to behave
  [ ] convert other integration tests from cucumber to behave
  [ ] merge branch in
[i] convert app to use html
  [x] find api call
    section list
      curl 'https://en.wikipedia.org/w/api.php?action=parse&format=json&page=Frog&prop=sections&disabletoc=1'
         | jq '.parse.sections|map({index, linkAnchor})'
    section HTML
      curl 'https://en.wikipedia.org/w/api.php?action=parse&page=Frog&prop=text&format=json&section=0'
         | jq '.parse.text["*"]'
  [x] figure out processing
    exclude
      table .reference, .references, .shortdescription, .note
    extract text from
      p, blockquote, ol, ul, dl
        entire contents of tag are indivisible chunk
        can have paragraphs within chunk for e.g. list items
        filter out empties
  [ ] update unit tests
    [ ] use beautiful soup to grab section
    [ ] html-sanitizer
    [ ] beautiful soup again to get text of paragraphs (and others)
      p blockquote ul ol dl
    [ ] remove square-bracketed citations as well as parenthesized expressions
  [ ] merge branch in
[ ] use studio voices
[ ] randomize story selection
  [ ] identify selected story in HTTP response
  [ ] use selected story to drive assertions
  [ ] weighted random selection of story (age in days is weight)
[ ] podcast on spotify
    [ ] spotify settings
    [ ] spreaker settings
[ ] consider sections when ordering articles within story
[ ] de-dupe articles
    [ ] by full reference (title and section)
[ ] weighted random selection of story
  - rank by age (oldest is heaviest)
  - number of articles in story (more articles are heavier)
[ ] iTunes
[ ] upgrade/remove libraries (Pydantic?)
[ ] upload in preview mode
    [ ] config flag (in terraform)
    [ ] flag controls uploading to spreaker in preview mode
[ ] Audible
[ ] re-assess voices
[ ] podcast on youtube
    [ ] channel
    [ ] spreaker settings
[ ] better error handling
[ ] exclude "list of" articles





import wikitextparser as wtp
import textwrap
source = textwrap.dedent(
    """
    It has a mean diameter of approximately {{convert|220|km|sp=us}} and
    contains about one percent of the mass of the [[asteroid belt]].
    """)
parsed = wtp.parse(source)
best = parsed.sections[0]
wtp.parse(best.contents).plain_text(
    replace_templates=lambda x: str(x.arguments[0].__dir__()))
