[i] scrape wikipedia using HTML, not wiki markup
  - too many random, quirky templates
  [x] separate branch
    html-based-scraping
  [x] set up Behave
    [x] replace Dockerfile.tests
      [x] /features
    [x] requirements-integration.txt
    [x] environment.py
      https://behave.readthedocs.io/en/stable/tutorial.html#environmental-controls
      [x] url lookups
      [x] poll wiremock ready
  [i] convert happy path integration test from cucumber to behave
      - convert from cucumber to behave as is
      [x] confirm all mocks up
      [i] mock news item
        - helpers that populate context
        - helper that syncs mock to context
        [x] headline with links
          [x] mock wikipedia headline
          [x] expected spreaker episode title (unabridged)
        [i] linked articles
          [i] expected wikipedia request info
          [i] mock wikipedia response info
          [i] expected tts request fragment
        [ ] tts
          [ ] mock tts response topic
          [ ] store mock in context
      [ ] trigger call to app
      [ ] expect calls to wikipedia
        [ ] headlines
        [ ] articles in context
      [ ] expect calls to tts
        [ ] combination of title and article fragments from context
      [ ] expect calls to spreaker
        [ ] abridged version of title from context
        [ ] mock mp3 data from context
        [ ] show, other metadata
  [ ] convert other integration tests from cucumber to behave
  [ ] uploading to spreaker without publishing
  [ ] merge branch in
  [ ] test
  [i] convert app to use html
    [x] find api call
      section list
        curl 'https://en.wikipedia.org/w/api.php?action=parse&format=json&page=Frog&prop=sections&disabletoc=1'
           | jq '.parse.sections|map({index, linkAnchor})'
      section HTML
        curl 'https://en.wikipedia.org/w/api.php?action=parse&page=Frog&prop=text&format=json&section=0'
           | jq '.parse.text["*"]'
    [x] figure out processing
      exclude
        table .reference, .references, .shortdescription, .note
      extract text from
        p, blockquote, ol, ul, dl
          entire contents of tag are indivisible chunk
          can have paragraphs within chunk for e.g. list items
          filter out empties
    [ ] update unit tests
      [ ] use beautiful soup to grab section
      [ ] html-sanitizer
      [ ] beautiful soup again to get text of paragraphs (and others)
        p blockquote ul ol dl
      [ ] remove square-bracketed citations as well as parenthesized expressions
  [ ] merge branch in
[ ] schedule release after run/upload
  [ ] release next 5am Eastern after run
  [ ] run noon HST
[ ] use studio voices
[ ] podcast on spotify
    [ ] spotify settings
    [ ] spreaker settings
[ ] consider sections when ordering articles within story
[ ] de-dupe articles
    [ ] by full reference (title and section)
[ ] weighted random selection of story
  - rank by age (oldest is heaviest)
  - number of articles in story (more articles are heavier)
[ ] iTunes
[ ] upgrade/remove libraries (Pydantic?)
[ ] upload in preview mode
    [ ] config flag (in terraform)
    [ ] flag controls uploading to spreaker in preview mode
[ ] Audible
[ ] re-assess voices
[ ] podcast on youtube
    [ ] channel
    [ ] spreaker settings
[ ] better error handling
[ ] exclude "list of" articles





import wikitextparser as wtp
import textwrap
source = textwrap.dedent(
    """
    It has a mean diameter of approximately {{convert|220|km|sp=us}} and
    contains about one percent of the mass of the [[asteroid belt]].
    """)
parsed = wtp.parse(source)
best = parsed.sections[0]
wtp.parse(best.contents).plain_text(
    replace_templates=lambda x: str(x.arguments[0].__dir__()))
